aliases: [[pavlov]] [[Pavlov]]
created: 20190310004211240
modified: 20190626203246763
tags: Rabbitholed [[The Dialectic]] T42T [[The Original Position]] Socialism CATI
title: 2019.03.09 - CATI: Pavlov

This is not the standard Pavlov in the literature. It aims to exploit though.

---

-=[ Rabbitholed ]=-

Nature adds random noise to the evolution of strategies, instances of prisoner's dilemmas, and various chaotic initial conditions across the iterations. The game theoretic answer can only be determined in context, one we can never know all the way down because we can't perfectly simulate ourselves. Reductive, probabilistic empiricism is just as much our plight as classic rationalism (yes, Saint Kant). Oscillations and invasions of competing strategies in the dialectic seem to offer no clearcut absolutely dominant strategy in all contexts; perhaps the correct strategy is a correct dialectical chain instead, though I smell an [[infinigress]] yet again. Perhaps even [[infinigress]]ion of [[VOI]]s in dialectical voting obtain, including zero-privacy perfect information. Even this cannot be calculated by the knowledge available to us.

There is much room for [[faith]]. It appears the classic-rationalistic correct strategy will be codifiable (though perhaps only with [[infinigress]], and then faith to our context) but so sensitive that we can barely generalize without taking some risks in guesswork. Pragmatized, there's "doing our best to get as close as we can."

One of my concerns about the Rawlsian definition of [[The Original Position]] is its employment of game-theoretically rational, egoistic personal utility maximizing agents behind the [[VOI]]. First, we can't be as omniscient or self-consistent as implied in the ideal, and it is a matter of faith that the [[fff]] will arrive at the same conclusions. At best, we get a simple bootstrapping heuristic, not the full [[infinite]] computer itself. I recognize the necessity of pragmatism to some [[dok]]. Second, this is nothing like Kant's vision whatsoever; the groundwork destroys mere consequentialism, and this difference has annoyed me. This may even be a game-ender, I'm not sure. I have long hoped the [[good]]will would be game-theoretically the preference of individualists behind the [[VOI]] as well, but there are some monkeywrenches.

Enter Hegel, and I'm immediately pleased to see the [[CI]] split into two principles of justice, master and slave in lexical ordering and dialectics.<<ref "m">> But, another problem appears: it seems like individualism and collectivism must be balanced against each other to make progress.<<ref "i">> Does this mean the agents behind the [[VOI]] weigh two prescriptions into equilibrium, one individualist and the [[other]] collectivist? Generating federalism [[irwartfrr]] for the justified Rule of Law is hard as fuck.

Enter Saint Aristotle after wrestling with Saint Kant, resulting in a realist's contextualist particularism embedded in [[The Categorical Imperative]], and you'll see evidence that we are responding to the vice we tend toward as a species: individualism. Risk-assessed, given the golden mean theory, we still have excellent evidence to think we should be selecting for collectivist moral motivation.

Enter [[Saint Plato]] who reinforces the moral perfectionism requirement to the N^^th^^ degree. I read Saint Kant as a Platonist.<<ref "s">> Perhaps passing the [[ROG]] just is the atomistic prototype of the molecular [[VOI]], or maybe these two gems of justice can be woven together. Saints Plato and Aristotle dialectically give a direct ideal [[End]] to Hegel's Grand [[Dialectic]], including unlimited, unconditional, universalizable requirements of [[The Moral Law]]. Describing [[The Infinite]] "isness" of the [[The Right]]'s "oughtness" aint easy, especially for we [[fff]].

Given this, how shall we game-theoretically define the agents behind the [[VOI]]? My intuition is that [[T42T]] is the obvious ideal first round draft, especially after considering our selfish dispositions as a species. Thus, prima facie, the prescriptive goal is to maximal cooperation; it has all the synergy and satisfies the golden mean approach.

It appears Rawls begs the question against [[The Categorical Imperative]] in choosing individualistic consequentialism to determine global utility given diminishing marginal returns and no utility monsters. Why should I accept egoism as being embedded at the heart of heuristic of [[The Moral Law]]? Perhaps the [[VOI]] already accounts for Hegel's collectivist requirement of the dialectic. Molecules have to built from atoms, right? I fear it's like using icicles as tentpegs in quicksand.

Problematically, the iterated prisoner's dilemma appears too simplistic (which might defeat everything here). It might be homo economic shooting spherical chickens in a vacuum. It doesn't match the real world is so many ways. Imagine I cooperate with you to rob a bank, but at the very end, I keep the doors locked on the getaway vehicle after you've already handed me the goods. It seems like trust is radically more stochastic and opportunistic in the real world. In many cases, one can sweep the sum utility of entire series of prisoner's dilemmas in a single well-timed defection. If I understand correctly, continuums and emergence give rise to a bunch of problems I don't know how to solve in this space too. 


[[T42T]] becomes just a rule-of-thumb ideally, but the hedged-conservative law practically. I'm not saying we aren't obligated to play it even when we know we are going to lose: far from it. Rather, we're forced to talk about the [[end]] to calculate toward, and while the absolutely correct answer might be almost anything in our particular context, the best answers and reasons we can come up with are just probabilistic. 




Imagine [[Saint Plato]] leading me out of the cave. He gives me some decent arguments, and we get 95% of the way there, and just before I see the sunlight, my doubt overrides and causes me turn back. [[Saint Plato]] could defect and drag me out, and only then would I realize he had actually be cooperating the whole time. Even the appearance of defection is deceiving.


The [[VOI]] attempts to distribute and generate [[The Good]] via decentralized intersubjective decision procedures.




In the beginning of the iterated prisoner's dilemmas, on average, we are likely to find dark-triadic might-makes-right State of Nature creatures generating social dominance through a semblance of Agar.io like growth. Eventually, enough players who cooperate in [[T4T]] take down these centralized powers.  



In some circumstances, Pavlov defeats [[T42T]]. Perhaps [[T42T]] is the answer to [[T4T]], but Pavlov seems to exploit forgiveness. Pavlov seems like nextgen always defect to me, depending upon the initial starting conditions of the game (but that just is the infeasibility of computing the chaos). It didn't theoretically bother me that [[T4T]] is likely going to win out, so this doesn't seem like it should either. The undecidability, uncertainty, incompleteness, halting, freezing, finite turing tape kinds of problems are icky as fuck.


Let's say you put on the Ring of Perfection, and it simply reprograms you to be the kind of creature that passes the [[ROG]] test when attempting to go behind the [[VOI]]. What happens? You enter into [[Golden Rule]] strategy which assumes everyone else behind the [[VOI]] is playing by the same [[Golden Rule]] strategy, since it must be exceptionless by definition. Yall do your voting, and figure out a gameplan for the world, but since it must be applicable to you outside the [[VOI]], it must include the calculations for all the particularities of your individual context. Does this really boil down to being an honest "effective altruist" via constantly engaging in the global utility calculation in your own position? That's what Rawls feels like to me.

Alright, what do I respect about [[TOP]]:

* Don't make an exception of yourself, but particularize your maxims.
* Heuristical, finite, embraces empiricism as best it can.
* Justice as radical cosmopolitan fairness; empathizing with [[Humanity]] as a whole.
* Relieving ourselves of our morally arbitrary characteristics (while still taking into account all characteristics, proportions, spectrums, as best it can); aims to get to the essence of our telos.


---
<<footnotes "m" "Markov chain?">>

<<footnotes "i" "It appears homo sapiens might be too individualistic, which must be weighed in the calculation of our particularized [[TOP]]. Star Trek's Borg, however, might be too collectivist, and their [[TOP]] would have to rebalance in the other direction on the golden mean.">>

<<footnotes "s" "Saint Sarkar is closer than Saint Sensen.">>