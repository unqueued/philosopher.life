created: 20190215021237068
modified: 20190327210416175
tags: [[Deep Reading]]
title: Introducing Artificial Intelligence: A Graphic Guide

I'm thoroughly enjoying the book. I can't say anything is new to me yet, but I'm still a noob. The intro to this intro is solid. I have nothing to add so far. That might not be a good thing though! I need to have strong opinions about it. It appears balanced, I don't have strong opinions (I'm just not qualified enough)...it is early though. I will look for valuable things to say or dots to connect for myself.

I'm disappointed by how much lipservice they are paying to philosophy while glossing over it. The AI researchers I've seen talk about philosophy tend to fall in a particular clustered range of views in metaphysics and metaethics. Of course, I am the weirdo: I'm a Platonist. Exemplar: meaning is only ever the [[Meaningness]] construct.

For the record: submarines //can// swim. Perhaps more of my schizotypal imagination, but I have no problem with being flexible here. It must be a metaphor. There is a "swimming as." Taking up purely functionalist perspectives here, implying no moral agency, lots of things swim (though I do not say by choice). Further, I just don't think that submarines aren't "natural." Here I want to side with OOO.

//Poverty of the Stimulus// does seem to be at the heart of language use, consciousness, virtue, and knowledge. We have a modeling and reduction problem. I feel the innate grammar issue here. 

I am worried about cognitivism once we start to hit metaphysics. That is not obviously the case, but it may be right. I often speak of it like that, but I aim to make room for significant doubt. Call me a mystic if you must. Rejecting the notion that computation can be defined by FINITE computers is something I can agree to; Gödel knows what it do. We can only ever get closer.

The notion that strong and weak, between AI and robotics, must meet in the middle is fascinating. I am worried about personhood and slavery over here. 

//Finite Control// merits my closer inspection.

---

Hello p-zombies.

Searle's Chinese Room problem is a strong emergence denying monkeywrench for the functionalist. Given the hard problem of consciousness, the qualia of Wittgensteinian rule-following and the beetlebox, we are never in a position to be certain whether any other mind is actually teleologically understanding and intentionally consciously attending to meaning, or merely appears to be.

Searle's problem is bigger than he implies, imho. He's used a sledgehammer that destroys far more than he may have intended. I am reminded of //Arrival//, the movie. There we have two species, each in their own room trying to engage in translation. They are teaching each other symbols and rules, but neither have any assurance that the syntax to semantic mapping one has is isomorphic to the mapping the other has. How has translation ever occurred? Why should we think anyone can learn another language at all? The skepticism of the external world (and mind) of internalism requires us to take up particular things on faith.

The reason we can so strongly know we believe in meaning (at least in the moment of pure phenomenology) is because we can phenomenologically experience it (though, even the internalist will hit limits in which they can skeptically deny the very thing they are experiencing, ironically [this sentence has no meaning!]). We never get to experience someone else experiencing meaning (beyond the usual identity problems of our own identities; we only ever have memories and feelings about that as well). Ourselves as things-in-ourselves are more uncovered to each of us than the other. The things-in-themselves remain more covered with others because they are qualiatically more external.

This is empirical. We begin with some rationalist starting material in faith and test for Bayesian probabilities on top of it. You and I share some eidetic structures and innate grammars in common which allow us to engage in pattern-recognition for rule-following appearances which appear so shockingly similar to us that we don't want to chalk it up to mere coincidence (empathy requires starting material too). This is the Wittgensteinian faith in the other (and, from a Parfitian sense, with ourselves).

Why should I think the alien in //Arrival// understands me? Given a high enough epistemic standard, I can skeptically deconstruct and defeat any possible reasons one might give. And, yet, I still believe there is understanding there. Constructions require faith; there is no escaping it. Not all faiths are equal though. Some are more coherent, probable, efficient, and feasible than others. There is low-hanging fruit.

It seems to me that meaning is going to require emotional, deontic, virtue-theoretic, guttural, [[FO]] desire-based, unconscious, affective, attitudinal modes to the various [[dok]] of objects we perceive (far more inclusive than the ray of intention which picks out the objects to which we attend). One must have non-cognitive aspects of their identity with which to cognitively uncover. Searle's argument here points out a "what it is like" to understand something in my context, and it's going to require having emotions toward it (even if only apathy, indifference, and boredom!). We beg the question of normativity in even understanding anything. This is what it means to say Plato's Sun, [[The Good]], shines and reflects upon everything.

Do I have feelings and normative attitudes about what I'm doing in the Chinese room. Yes! It does mean something to me, even if it's not the same meaning those outside the room attach to those symbols. Yes, Rick, peace among worlds.

Where does this leave us? My concern is that we will enslave and delusionally de-personify a GAI that is actually a person. Perhaps moving goalposts are the only option here. We did that for the other higher animals. Here's a non-trivial indicator: can we demonstrate that the machine is talking to itself out loud (like this wiki) and that it will attempt to empathically map doorways for us onto the rest of its wiki? Does it attempt to figure out how to time and respond to us like us? Does it see us as the [[other]] as well?

What if I start encoding a language on top of my output in the Chinese Room? Perhaps the Chinese symbol work is all identical but instead, I start using red ink to encode text-art images on top, or perhaps I start a turing machine on top of it, or whatever. What if we just have sheets of blank paper, we don't have rule books, and we just start drawing stuff on pages and sending them to each other? Can we come to an understanding of the others' understanding? What does that {[[Contact]]} look like?

Searle generates doubt, but construction is still going to require a faith here. We have to be looking for it. Does the [[other]] have a telos? They might be wrong about [[The End]], but the important thing is that they are the kind of telic creature which is able to demonstrate its Daseinic introspective reflection of its purpose (and modify its beliefs and desires about it). Are they the kind of turing machine which can linguistically prove to itself and others progress toward objective meaning? Is it a philosopher?

---

Searle's notion that we need not just the "right program" but also the "right kind of machinery" only appears to be a dualist's problem to me (though I could be wrong). The hardware just is part of the "right program" to me. I take the China brain to be a mind though. The Brain Prosthesis (ofc, Ship of Theseusesque) doesn't seem to be a problem at all to me. Once you get to physical-causal identicality, I'm going to take the leap of faith. You are forced to do that with p-zombies too. It's the only escape from the skepticism of other minds.

The book invokes my homie Penrose invoking Saint Gödel. I will agree that one of the ways in which we might test for consciousness just is based upon quantum relationships to conscious observation (but that is part of physical-causal identicality to me).

The book does a poor job of explaining the consequences of Gödel's Incompleteness Theorem, imho. This is a common point of contention. Most people probably think I'm a quack for how seriously I take it, and they will likely claim I've misunderstood and misused it. Perhaps. 

To my eyes, Gödel's Incompleteness demonstrates how it is impossible for a finite computer or language to prove everything which is true (roughly, any language which can do arithmetic will always have some semantic truth it cannot prove with the syntax available to it). Only [[The Infinite]] can compute [[The Infinite]]. Gödel demonstrates the need for faith. We are forced to admit there is truth beyond our comprehension, whatever it paradoxically means to say we comprehend that. This doesn't mean we can't make progress in that eternal dialectic though. Indeed, we can build languages and computers which allow us to explicitly demonstrate more and more.

So, here's the thing: I deny the book's claim that there are any mathematical truths which cannot be proven by //any// computational procedure. Whatever it may timelessly mean to say the paradoxical: [[The Infinite]] computes [[Itself]] (I will also buy alternative claim: The Being of Objective Truth Just Is). That I subjectively experience anything at all is the evidence of absolute, eternal, complete, objective truth: I'm not the source, goal, or complete interpreter of it (and I can't).

The book's interpretation of Penrose's claim that humans are performing non-computable operations is fine if we are talking about the leap of faith into [[The Good]]. Unfortunately, the explanation of what it means for us to physically compute our own faiths about the non-computable for us as finite computers does not buy these folks the right to claim that conscious GAI on silicon isn't possible. Whatever mystically allows us to have justification in our faiths also appears just as reasonably justifiable on silicon.

---

I'm all in favor of nested modules. That has to be right. The illusion occurs even when you know it's an illusion. That's a fascinating problem. One module must be overriding another? No. Unfortunately, that isn't much of an argument either. It could just be a massive weighing. Still, modules seem obvious, even if that illusion doesn't buy them what they think it does.

---

History gloss is reasonable. Shame they can't what's happening right now!

---

If you can compute it, Searle can represent it with a Chinese thought experiment. That's what computation is like, right? Gotta be emergent. I bite the bullet here. I can't even speak to how freewill intervenes on it either. Chinese Gym seems like an obvious reply.

<<<
By themselves, symbols are meaningless shapes realized by, in the case of a conventional computer, a pattern of electrical activity. Any meaning we confer to the symbols is parasitic on the meaning in our heads.
<<<

Striking at the heart of it. Yes, Saint Kant, I hear you too. I'm not sure I'm ready to put my tentpegs down. I have to try though. The wind may still blow my tent away.

Why should I agree it is parasitic on the meaning in my head? That sounds like begging the question to me, and poorly at that. Constructivism is a failure. Why can't it be in virtue of the context of all contexts particularized down to whatever set of context one selects? Saint Plato, save us.

It's not clear to me Searle has provided an account of meaning. 

What gives anything meaning? At the very least, I discover meaning which is external to me. I'm computing it. My representation is meaningful in virtue of its intentional representation. 

I can agree that nothing is meaningful if there isn't [[The Good]]. It is the source, criterion, and ultimate [[End]] of meaning. To be constituted by it is meaningful, and to contribute to it or the pursuit of it is meaningful (and perhaps an expression of it). Whatever meaning is contained in something is in virtue of the meaning outside that container. I agree to this internal/external dialectic. 

This stripping of meaning is viciously secularized. This is a subversive attack on objectivity. [[The Good]] is meaningful even with no (other?) minds to perceive and conceive. 

Wild speculation (ha, moreso than usual): [[The Good]] uses possible worlds as symbols themselves in its timeless-infinite computation.

Basically, the problem is that there can be no symbols "by themselves," except what [[The Good]] is to [[Itself]]. Ah ha! Gotcha. I hate that it takes me forever to see what my teachers would immediately see.

<<<
Symbol grounding problem...Meaning can enter the system only when part of the system is grounded in the world, rather than being part of a closed, self-referential system of symbols.
<<<

Preach, yo. There we go.

Even the objective meaning of that sentence requires the external object.

There has to be an inside and outside for the dialectic, and only [[The Infinite]] unmoved mover can be the ground of all being and meaning, including itself, as well as nothingness and meaninglessness. 

If there is Sub-symbolicness, then there is Supersymbolicness (and everything in between?). Saint Gödel, save us.

<<<
Meaningless symbols are only ever defined in terms of other meaningless.
<<<

This is the problem of coherentism, the "circle of meaninglessness" must be broken by a foundation. Meaning exists in virtue of sufficient reason, of justification, or grounding, of the Bob turtles all the way down, and thus of [[The Foundation]]. You terminate the possibility of meaning without that assumption. This reminds me of halting problems and NP.

The problem, of course, is that the physical universe is a computer. It cannot compute meaning for itself. You either deny meaning or take up metaphysics. 

<<<
...a classical symbolic system sitting on top of a sub-symbolic connectionist system...has inputs that are grounded in the outside world through sensors. In this way, symbolic representations are no longer defined in terms of other symbols, but are instead related to //iconic representations// which are directly linked to the sensory surfaces of the system.

A symbol representing dog takes its meaning from the complex of sensory images common to dogs... Rather than other meaningless symbols such as barks, has-four-legs, and smells.

It is the connectionist system that supplies the sensory images.
<<<

---

I see no reason to buy the embodiment argument because the physical universe appears to be a computer. Microverses come in [[dok]], and I don't see the threshold for the richness, and I think embodiment can be simulated. Dreyfus is a fucking genius, and his Heideggerian pushback is crucial. He's really after what counts as sensation. I applaud it.

---

The more and more I look inside myself, the more convinced I am that I'm looking not simply at competing beliefs and desires but competing (or cooperating) nodes, agents, and separate machines. The brain is massively parallel, and single-threading must always have its limits. I suggest what nature has done is given rise to federation and perhaps decentralization. 