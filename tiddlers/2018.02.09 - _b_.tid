created: 20180209165547016
modified: 20200209174854506
modifier: h0p3
tags: /b/
title: 2018.02.09 - /b/

<<<
I had the most satisfying Eureka experience of my career while attempting to teach flight instructors that praise is more effective than punishment for promoting skill-learning. When I had finished my enthusiastic speech, one of the most seasoned instructors in the audience raised his hand and made his own short speech, which began by conceding that positive reinforcement might be good for the birds, but went on to deny that it was optimal for flight cadets. He said, “On many occasions I have praised flight cadets for clean execution of some aerobatic maneuver, and in general when they try it again, they do worse. On the other hand, I have often screamed at cadets for bad execution, and in general they do better the next time. So please don't tell us that reinforcement works and punishment does not, because the opposite is the case.” This was a joyous moment, in which I understood an important truth about the world: because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them. I immediately arranged a demonstration in which each participant tossed two coins at a target behind his back, without any feedback. We measured the distances from the target and could see that those who had done best the first time had mostly deteriorated on their second try, and vice versa. But I knew that this demonstration would not undo the effects of lifelong exposure to a perverse contingency.<<ref "2020.02.09-1">>
<<<

---

In the first round of our CI computation, we might say that we think of a world in which everyone plays by the rules of your maxim. This, however, is not consequentialist or particularist enough. That's just not the context we live in. I suggest this is just a first round, but not the ultimate calculation because we need context.<<ref "2020.02.09-2">>

Pragmatizing the CI, particularizing maxims to include more specific contexts.


---
<<footnotes "2020.02.09-1" "I have no idea why I liked this or felt it was valuable. It does feel plenty dialectical, but I don't see the eureka. What am I missing now?">>

<<footnotes "2020.02.09-2" "The actual computation is not something anyone has ever given a completionist account of. I think this causes many people to deny it can be done or should even be attempted. Perhaps this is an AI kind of problem.">>