created: 20180516204256149
modified: 20180516204257028
title: 2010.12.02 -- Symbolic Logic Notes



Notes – August 31, 2010

Function’s bring about more complex constants or referring expressions.

father(me) = My father

mother(father(me)) = My father’s mother = Mother of my Father

+(2,3) = 5

Both predicate and function symbols have Arity.

Infix notation is acceptable when it is conventional -- e.g. 2+3 = 5

Term = intuitively, it is an expression that serves to pick out an individual object. I.e. a Referring expression.

Individual constants are part of the set of all Terms. Individual constants are simple terms.

There are two types of terms. 1.) Simple (individual constants), and 2.) Complex.

Complex terms are the results of function symbols applied to a term. Obviously, complexity ranges.

Father(gwb) is different from father(gwb). Note the difference in capitalization. Both are well-formed expressions in our formal language, but they are very different. The latter is a function, also a complex term. Its job is to refer. The former contains a predicate (and a subject); it is an atomic sentence. It has a truth value.

“A is a cube” can be written as Cube(a).

“C is between a and d” can be written as Between(c, a, d). Choosing the order of terms matters, particularly as these are related in the sentence. When translating, try to stay as close as possible to the surface grammar that you are translating. So, while Between(c, d, a) is also true, Between(c, a, d) matches the original grammar better.

From 1.12:

    Claire’s father is taller than Max’s father.

    John is Max’s father. (Apparently, better than “John is identical to Max’s father”)

    Claire is taller than her maternal grandmother.

    Max’s maternal grandmother is taller than his paternal grandmother.

    Melanie and Claire have the same mother.

Argument = a collection of statements, one of which is called the conclusion. It is intended to follow from (be a consequence of) or be supported by the remaining statements. The remaining sentences are known as the premises.

There are signals in normal English for which argumentative sentences are which.

Conclusion: therefore, thus, hence, so

Premises: since, after all, because

    All humans are Mortal. Socrates is human. So, Socrates is Mortal.

    Lucretius is human. After all, all humans are mortal and Lucretius is mortal.

Fitch format down below. Note the “fitch bar” which separates the premises from the conclusion.

    | All humans are mortal.

| Socrates is human.

|---

| Socrates is mortal.



    | All humans are mortal.

| Lucretius is mortal.

|---

| Lucretius is human.

(Obviously, 2 isn’t valid)

An argument is logically (aka, deductively) valid iff (if and only if) its conclusion must be true if its premises are true. Thus, it is also impossible for the premises to be true and the conclusion false.

The conclusion of a logically valid argument is said to be a logical consequence of the argument’s premises.

Logical consequence and Logical validity are brothers.

Modal logics have to do with possibility and necessity. You must think about whether or not it is possible for the premises to be true and the conclusion false, independent of the actual truth or falsity.

Actual worlds and possible worlds (possible truth values).

    George Bush is President and Hillary Clinton is Secretary of State. Therefore, Hillary Rodham Clinton is Secretary of State.

If you suppose the premise is true, then the conclusion MUST be true. This is logically valid. Notice that the premise is actually false, but the conclusion is true.

    All humans are mortal. Obama is human. Therefore, Obama is mortal.

Valid. Actual truth values: true as well.

    All humans are mortal. Obama is immortals. Therefore Obama is not human.

Valid. 1 premise and 1 conclusion is actually false though.

Logical validity and actual truth values do not track each other.

Moving from actually true premises to an actually false conclusion is time where “tracking” can show the invalidity of an argument.

An argument is “sound” iff both 1.) it is logically valid and 2.) its premises are all true.

True in, true out = Truth preservation.

Soundness is practical.



Notes – September 2, 2010

A proof is a step by step demonstration that one statement (say S) is a logical consequence of some statements (say p1,….,pn).

Note that individual statements can easily prove themselves.

Formal Proofs must be in a formal language using explicitly specified rules.

Informal Proofs do not necessarily need explicitly specified rules and methods. Linguistic competence (syllogistic logic) is much like it.

Formal and informal only differ in style only, but don’t differ in rigor. Rigor, meaning, each step of the proof follows from previous steps by/of necessity. It is part of truth preservation. Start with true stuff necessarily ends up with true stuff.



Cube(b)

c = b

--

Cube(c)



[Sp’ = suppose (or assume)]

Sp’ Cube(b) and c=b.

We need to show Cube(c)

Informal proof:

Since c=b, c and b have exactly the same properties (are identical) , but b is a cube (i.e. Cube(b)), and since being a cube is a type of property, (so) c is also a cube (Cube(c)).

Formal proof (fitch style):

    Cube(c)

    c=b

--

3. Cube(b) = Elim: 1,2



Indiscernability of identicals (Leibniz principle):

If a=b, then a and b have exactly the same properties.

Things which are identical are indiscernible (you can’t tell them apart). They are the same thing.

This is basis of the identity elimination rule in our formal language.



Identify is reflexive. Everything is self-identical. a=a

This principle underwrites the “identity introduction” (Id intro) rule.



Identity is symmetric and transitive.

Symmetry: for all a and b, if a=b, then b=a

Transitivity: for all a,b, and c; if a=b, and b=c, then a=c



Informal argument for symmetry:

Let a and b be arbitrary. Sp’ a=b. Show: b=a

By reflexivity of = (identity), we have a=a. But, by the indiscernibility of identicals, a and b have exactly the same properties. So, it follows that b=a.



In our block-language, SameSize is reflexive, symmetrical, and transitive.

Every block is the same size as itself, if you have two blocks of the same size, then they are symmetrical, and if you have a,b,c with Same Size between any 2 sets of these, then all 3 are transitively the same size.

If you have Reflexitivity, Symmetry, and Transivity, then you have an Equivalence (of relation in these cases).

Inverses: consider Larger and Smaller. Example of an inverse relationship:

Larger(a,b) iff Smaller(b,a)

Deductive systems are necessary for presenting formal proofs.

We will be using “script” “F”. This is a Fitch style deductive system. (Hence, the “F”)

| 1. P1

| 2. P2

| .

| .

| n. Pn

|--

| n+1. S1	(justification for n+1)

| n+2. S2 (justification for n+2)

| .

| .

| n+k. Sk

| n+k+1. S



Justification shows the legitimacy of writing down the line; the application of the rules.

n+1 and n+2 are intermediate conclusions. They bridge the gap between what you are given as premises and what you are trying to prove in the end.



Rules of Script F:

    Identity Introduction or abbrev. (=Intro)



    | k. n=n

Where n is any term.

At any point in the argument overall, you may assert the above.



    Identity Elimination or (= Elim)







| k. P(n)

| l. n=m

    | q. P(m) = Elim: k, l

P(n): any sentence in which the term n appears.

Order matters.

Replace occurrences of n in P(n) with m – the proof isn’t the other way around

P(n) is the property statement

State the property statement first in “= Elim: k, l” assuming “k” is the property statement.



    Reiteration Rule or (Reit)

| k. p

| .

    | l. p Reit: k



| 1. a = b (b = a, symmetry of identity)

|--

| 2. a=a (Property statement) =Intro

| 3. b=a =Elim: 2, 1



| 1. SameRow(a,a) (show SameRow(b,a))

| 2. b=a (can’t replace any ‘b’ with ‘a’ because no b’s here)

|--

| 3. b=b =Intro

| 4. a=b =Elim: 3, 2

| 5. SameRow(b, a) =Elim: 1, 4



Property statements are the statement in which are replacing occurences.



Notes – September 7, 2010

Nonconsequence (can’t always prove this).

To prove S is no a consequence of P1…Pn:

Show it’s possible for the P’s to all be true and S to be false. You can do this by showing a counterexample.

A counterexample is a possible situation/circumstances/world in which P1…Pn are all true and S is False.



| Joe Biden a politician. T

| Few politicians are honest. T

|--

| Biden is not honest. F

Let the world be such that Biden is a politician, and Few politicians are honest, and Biden among the honest politicians. (This is the specification, later we’ll need to do verification.)



2.13 on pg 53.

| SameSize(a, b)

| Larger(a, c)  Smaller(c, a)

| Smaller(d, c)

|--

| Smaller(d, b)



Sp’ ‘a’ and ‘b’ are the same size, ‘a’ is larger than ‘c’, and ‘d’ is smaller than ‘c’. Show: ‘d’ is smaller than ‘b’.

Since ‘a’ is larger than ‘c’, ‘c’ is smaller than ‘a’.

So, by transitivity of ‘Smaller than’, ‘d’ is smaller than ‘a’.

But, ‘a’ and ‘b’are the same size, hence ‘d’ is smaller than ‘b’.



Boolean Connectives/Operators:

Negation ¬

It is not the case that…

Not or un-

You’ll always want to know a language’s syntax and semantics. Syntax is how a symbol works with language you already have to form new expressions. Syntax is grammar. Semantics asks, under what conditions is using that new piece of language true or false?

Syntax for ¬:

If p is a sentence, then so is ¬p.

Semantics for ¬: ¬P is true iff P is not true. P is false.

P | ¬P

---- ----

T | F

F | T



Truth functional connectives.

A ‘literal’ is a sentence which is either atomic or negated atomic.



Conjunction ^ (or &, but not in this class) or ∧

And, but, moreover

Bob and Jim are tall.

Tall(bob) ∧ Tall(jim)

‘Tall(bob)’ is a conjunct (same for ‘Tall(jim)’).





Same rested and listened to music.

Rested(sam) ∧ Music(sam)



Jill is a tall woman.

Tall(jill) ∧ Woman(jill)



Not every use of “and” is the conjunction.

Same brushed his teeth and (then) went to bed. “and” has a temporal meaning beyond mere truth functional conjunction.

The truth functional conjunction, you should be able to flip the order of the conjuncts and arrive at the same meaning.



Syntax for ∧: If P and Q are sentences, then so it P∧Q

Semantics for ∧: P∧Q is true iff both P is true and Q is true.

P Q | P∧Q

---- ---- ----

T T | T

T F | F

F T | F

F F | F



Notes – September 9, 2010

Disjunction - ∨ - or

Bob or Kim is Married.

Married(bob) ∨ Married(kim)

Inclusive or

One or the other or both

Disjuncts are joined by a Disjunction to make a Disjunctive sentence

Bob may have either soup or salad with his meal.

(Soup(bob) v Salad(bob)) ^ ~(Soup(bob) v Salad(bob))

If P and Q are sentences, then so is P v Q.

Sentences for ∨: P v Q is true iff at least one of P, Q is true.







P Q | P v Q

--- --- --- ---

T T | T

T F | T

F T | T

F F | F



Grouping – “groupers”

(), [], {}

Ted is dead and Bob is tall or Kim is home.

    Dead(ted) ^ (Tall(bob) v Home(kim))

    (Dead(ted) ^ Tall(bob)) v Home(kim)

Dead(ted) ^ Tall(bob) ^ Home(kim)  Doesn’t need groupers

Dead(ted) v Tall(bob) v Home(kim)  Doesn’t need groupers



    Bob kicked the ball.

    The ball was kicked by Bob.

These sentences are logically equivalent. They are logically equivalent if they necessarily have the same truth value.

(DN) – Double Negation - ~~P <==> P

(DM^) - Demorgan’s Law of conjunction - ~(P ^ Q) <==> ~P v ~Q

(DMv) – Demorgan’s Law of Disjunction - ~(P v Q) <==> ~P ^ ~Q



Good translation preserves meaning. It must match as closely as possible. Meaning of a statement are its truth conditions. Truth conditions are the circumstances under which the statement is true. You are looking for logical equivalence between that which is translated and the translation.

A translation of a sentence S1 into a sentence S2 is correct if S1 and S2 have the same truth conditions.

Any possible situation (not just one or some) in which one is true the other is true as well. (Logically equivalent)



Stylistic considerations:

    Match the surface syntax as closely as possible.

    Maximize naturalness (even colloquial)



    Not either/Neither, Nor ~(P v Q)

    Either not ~P v ~Q

    Not both ~(P ^ Q)

    Both not ~P ^ ~Q

1 and 4 are equivalent; 2 and 3 are equivalent.



    Neither e nor a is to the right of and to the left of b.

~((Rightof(e, c) ^ Leftof(e, b)) v (Rightof(a, c) ^ Leftof(a, b))

    Either a is small or both c and d are large.

Small(a) v (Large(c) ^ Large(d))



Notes – September 14, 2010

Either the President supports campaign reform and the House adopts universal health care or the Senate approves missile defense.

(S(a)^A(b))vM(c)

Not both Hertz and Avis rent limousines.

~(R(h)^R(a))  Demorgan’s, ~R(h) v ~R(a)

Both hertz and Avis do not rent limousines.
~R(h)^~R(a)



Either Motrin or Advil cures headaches.

C(m)vC(a)



Not either Mylanta or Pepcid cures headaches.

~(C(m)v(C(p))



Neither Mylanta nor Pepcid cures headaches.

~(C(m)v(C(p))



Either Mylanta or Pepcid does not cure headaches.

~C(m) v ~C(p)



~, ^, v -- these are truth-functional connectives. The truth values of the atomics of a sentence containing Boolean connectives define the truth value of those connectives.

It is necessarily the case that (or, “it is necessary”) – modal operators. Beyond truth value of the sentence in the actual world, but even possible worlds. This isn’t a truth-functional connective. The word “because”, likewise, isn’t a truth-functional connective because it depends on more than the current truth values of the atomics in the sentence using the word “because”.



Logical Statuses (Stati?, Stats?):

Logical consequence

Logical truth

Logical equivalence



Truth Tables help define the Statuses?

A sentence S is a logical truth iff it is logically impossible for S to be false. Aka. Necessary Truth (and Logically necessary truth).

A = A

Tet(a) v ~Tet(a)



Physical Possibility: doesn’t violate physical law

TW Possibility (Tarski World Possibility) specific to our book: can be built within the Tarski block world.

Facts:

    Every TW-possible sentence is logically possible.

    Some logically possible sentences are not TW-possible.

        ~(Tet(b) v Dedec(b) v Cube(b)) (e.g. can’t make a sphere)



Truth Table for a Sentence: A truth table for a sentence P is an arrangement of truth values that shows the truth value of P in every possible situation as determined by the truth values of the atomic sentences occurring in P.

Main Connective: The main connective of a non-atomic sentence is that connective such that no other connective operates on a larger (i.e., more complex) part of the sentence than it does.

S, Q are true; R is False



(S ^ ~Q) v ~~R

T T F

F T .

F .

F .

F .



Put in values for atomics, Rotate between negations and connectives until you reach the main connective. The main connective’s truth value will tell you the full Truth Value of the sentence.

If there are n different atomic sentences occurring in P, then the truth table for P will have 2­n rows.



S Q R || (S ^ ~Q) v ~~R

T T T || F F T T F

T T F || F F F F T

T F T || T T T T F

T F F || T T T F T

F T T || F F T T F

F T F || F F F F T

F F T || F T T T F

F F F || F T F F T





Notes – September 16, 2010



Tet(b) || Tet(b) v ~Tet(b)

T || T F

F || T T



This is a tautology. Logical truth which can be shown via truth table (some logical truths can’t be demonstrated this way).



A: Cube(a)

B: Cube(b)

C: Cube(c)



A B C || (A ^ B) v ~C

T T T || T F

T T F || T T

T F T || F F F

T F F || F T

F T T || F F F

F T F || F T

F F T || F F F

F F F || F T



Contingent sentence because the truth values of atomics matter to which world we are in.



Tet(b) || ~[Tet(b) v ~Tet(b)]

T || F T F

F || F T T





Something which is false is every situation is necessarily false or contradiction.

Every logically necessary sentence is TW-Necessary.



Convention for truth tables on chained “or” or “and” is to group from the left

(((A ^ B) ^ C) ^ D) ^ E



Every tautology is logically necessary.

Some logical necessities are not tautologies.



Truth tables are insensitive to “larger” or Identity statements. They can’t show all logically necessary statements because of this, only tautologies.



TT=Truth Table



A sentence S is TT-possible iff S is true on at least one row of its truth table.



TT-Possible is “consistent” or internally consistent, but this is only a subspecies of “consistencies”

TT-Necessary = Tautology

TT-Impossible=Contradiction



Joint truth table: a truth table built for more than one sentence.



A B || ~(A ^ B) | ~A v ~B

T T || F T | F F F

T F || T F | F T T

F T || T F | T T F

F F || T F | T T T



Compare the truth values of each sub-table.



The above table shows that both sentences are tautologically equivalent because they agree (have the same truth value) on every row under the main connective in their truth table.

Tautological equivalence is a subspecies of logical equivalence.



Truth tables tell you about the meanings of truth-functional connectives, which is why we can see proofs of tautology in TTs.



Every Tautological equivalent pair of sentences is logically equivalent.

Some logically equivalent pairs are not tautologically equivalent pairs.



S is a logical truth iff S is a logical consequence of any set of sentences.

S and S’ are logically equivalent iff S is a logical consequence of S’ and vice versa.



TT give us a way to define the notion of tautological consequence.

S is a tautological consequence of P1…Pn iff the joint truth table for S and P1…Pn has no row where each of the P’s is true and S is false.



Tautological consequence is a subspecies of logical consequence.



Tautology = Truth Table = Truth-functional connectives relationships are fully understood/described



Tautological consequence, equivalence, and truth relate to each other in the same that way Logical consequence, equivalence, and truth relate to themselves.



Tautological consequence relates to logical consequence in the same way that Tautological equivalence relates to logical equivalence in the same way that Tautological truth relates to logical truth.





Notes – September 21



Any instance of tautological consequence is an instance of logical consequence.



Some instances of logical consequences are not instances of tautological consequence. E.g. a=b & b=c, therefore a=c. This is a logical consequence, but not tautological consequence because tautological consequence can be captured in truth tables and can only use truth-connectives. Some logical consequence use non-truth-connectives.



S is a tautology iff S is a tautological consequence of any set of premises.



S and S’ are tautologically equivalent iff S and S’ are tautological consequences of each other.



Chapter 5



An inference step is a move from one or more sentences to a sentence in a process or pattern of reasoning. E.g. P ^ Q -> P



Valid inference step just in case it is truth preserving. If the sentences you start from in the inference step, then so must the sentence you step to (the outcome of that inference step). Truth in, truth out; Truth preserving.



3 Simple valid inference steps:



    From a conjunction of any number of sentences, one may infer any one of the conjuncts.

From P1 ^ P2 ^ Pn, infer Pi (where ‘i’ is between 1 and ‘n’)

Conjunction Elim rule (^ Elim)

    From any number of sentences, one may infer the conjunction of these sentences.

From truth of all of P1, P2, Pn -> infer P1 ^ P2 ^ Pn

Conjunction intro rule (^ Intro)

    From any sentence, one may infer a disjunction of any number of sentences containing that sentence as a disjunct.

From P, infer P v Q, P v Z, P v Q v Z v S

Disjunction intro rule (v Intro)



Every step of a proof (formal or informal) should be easily understandable and significant.

Easily understood -> Easy to see the step is valid. Obviously, this is audience sensitive.

Significance must move the proof forward (metaphorically speaking).



Proof by Cases.

(Cube(c) ^ Small(c)) v (Tet(c) ^ Small(c))

--

Small(c)



p.f.. Sp’ (Cube(c) ^ Small(c)) v (Tet(c) ^ Small(c))

Show: Small(c)

There are two cases to consider.



Case 1 – Cube(c) ^ Small(c) -> Small(c)

Case 2 – Tet(c) ^ Small(c) -> Small(c)

This exhausts the possibilities. So, Small(c)



Proof by Cases: To prove S from P1 v P2 v Pn: Show that S is a consequence of each Pi (where ‘i’ is between 1 and ‘n’)

Proof by Cases is specific to disjunctions. Proof by cases underwrites disjunction elimination (v Elim)



Notes – September 23, 2010

To prove S from P1 v P2 v Pn, prove S from each Pi

Indirect proof/proof by contradiction/ reduction ad absurdum

To prove ~S from sentence P1...Pn, assume S and derive a contradiction.

Contradiction = a sentence which is necessarily false.

P1 T

. T

Pn T

S ?

--

X F

Thus, S must necessarily be false. We know which one to blame, since the P’s are already taken to be true in this study, only S is left, and it must be False. QED, ~S is true!

Contradiction must be necessarily false from these.



Show: B != C follows from Cube(c) v Dodec(c), and Tet(b).

Assume:

Cube(c) v Dodec(c)

Tet(b)

Assume for reductio that b = c. (I,e, ~[b!=c])

Since Cube(c) v Dodec(c), there are two cases to consider.

Case 1: Cube(c)  Then we have Cube(c) and Tet(b). But, since b=c, we have a Tet(c). It is impossible for Cube(c) and Tet(c) to be true at the same time.

Case 2: Dodec(c)  Then we have Dodec(c) and Tet(b). But, since b=c, we have Tet(c). It is impossible for Dodec(c) and Tet(c) to be true at the same time.

Since this exhausts the cases, the premises plus b=c lead to impossibility. So, b!=c

This is an inconsistent set.

⊥: Absurd, Surd, bottom, falsity, the false, contradiction

Fact: S is a contradiction iff ~S is a logical truth



Chapter 6

We are expanding F in this chapter.



Conjunction elimination -- ^ Elim

|k. P1 ^ Pi ^ Pn

|.

 |n. P1 ^Elim: k



Conjunction introduction – v Intro

|k. P1

|.

|km. Pm

|n. P1 ^ P2^…^Pm ^Intro: k., k2…km

The references for ^Intro need to match the order in which you put the conjuncts.



Examples:

|1. A ^ B ^ C Prove: C^B

--

|2. B ^Elim: 1

|3. C ^Elim:1

|4. C^B ^Intro: 3, 2



Beware of groupers, you may need to remove them to avoid ambiguity.

Example:

|1. A v B

|2. C

|--

|3. (A v B) ^ C ^Intro: 1, 2



Disjunction Introduction – v Intro

|k. Pi

|.

|n. P1 v…v Pi v… v Pn	v Intro: k



Disjunction Elimination – v Elim

Subproof: A proof that occurs inside a larger proof



|1. (A ^ B) v (C ^ D)	Show: B v D

| |2. A^B

| |3. B ^ Elim:2

| |4. B v D v Intro:3

| |5. C ^ D

| |6. D ^ Elim: 5

| |7. B v D v Intro: 6

|8. B v D v Elim:1, 2-4, 5-7

Notes – September 28

Negation Elimination -- ~Elim

|k. ~~P

|.

|n. P ~Elim: k



Negation Introduction -- ~Intro

| |k. P

| |.

| |n. ⊥

| n+1. ~P ~Intro: k-n



Surd Introduction -- ⊥ Intro

|k. P

| .

| l. ~P

| .

|n. ⊥	⊥ Intro: k, l



Non-negated line goes first.



Surd Elimination -- ⊥ Elim

|k. ⊥

| .

| n. P ⊥ Elim: k

Where P is ANY sentence of the language. This is similar to a Counterfactual.



Proof Strategies:

    Try an informal proof

    Think about what the sentences actually mean

    Work backwards by identifying the middle/intermediate goals.



You should (not can) only start a subproof when you know what rules you wish to employ and what you intend the last line of that proof to look like.

|~P v ~Q

|--

| ~(P ^ Q)



Sp’ ~P v ~Q. Show: ~(P ^ Q)

Assume for reduction: P^ Q. Then both P and Q are true. Now consider either two cases:

Case 1: ~~P holds. Then P and ~P 

Case 2: ~Q holds. Then Q and ~Q 



Notes – September 30, 2010

| Dodec(e)

| Small(e)

| ~Dodec(e) v Dodec(f) v Small(e)

|--

| Dodec(f)

Not deductively valid, where is the counterexample?

Consider a world in which e is a small dodec and f is a cube.

The First premise is true in this world, since e is a dodec.

The Second premise is also true in this world, since e is small.

The Third premise is true in this world, since e is small and Small(e) is one of the disjuncts of this premise.

But, the conclusion is false in this world because f is not a dodec.



Notes – October 5, 2010

    De Es

    Trans

    TT

    Proofs/Counterexample



Material Condition -- →

If P, then Q

P only if Q

Q if P

Q if only P

Q provided that P

P is sufficient for Q

Q is necessary for P

The above are written as: P  Q



Syntax for 

If P and Q are sentences, then so is PQ

This is called a conditional.

Within a conditional, such as P  Q, P is the antecedent and Q is the consequent.



Semantics for 

P  Q is true if either P is false or Q is true.

P Q | P  Q

T T | T

T F | F

F T | T

F F | T



Notice how the only time P  Q is false is when P is true and Q is false. False P will always make P  Q true.



If Max had been at home, then Carl would have been there too. (This isn’t a material condition because the antecedent is false, and the entire conditional is false, which is not possible according to the truth table for material conditional).

Indicative mood and subjunctive mood will demonstrate which English sentences are material conditionals and which aren’t.



P unless Q – this is written as -- ~Q  P

Ted will die unless Bob helps him.

If Bob doesn’t help him, Ted will die.

~Helps(bob, ted)  Dies(ted)



Corresponding Conditional (Associated conditional)

With any argument, you can write a conditional which corresponds to it.

|P1

|P2

|.

|Pn

|-

| C

(P1 ^ P2 ^ … ^ Pn)  C



An argument is deductively valid iff its corresponding conditional is a logical truth.



Material Biconditional -- ↔

P iff Q

P just in case Q

P is necessary and sufficient for Q

P ↔ Q

P and Q are logically equivalent iff P ↔ Q is a logical truth.



Syntax for ↔

If P and Q are sentences, then so is P ↔ Q



Semantics for ↔

P ↔ Q is true iff the truth values of P and Q match



P Q | P ↔ Q

T T | T

T F | F

F T | F

F F | T

Where they match, obviously, the biconditional is true. Where they don’t have matching truth values, this statement is false. Material biconditionals have the same truth values and are logically equivalent.



P ↔ Q is logically equivalent to (P  Q) ^ (Q  P)



Conversational implicatur

(from Paul Grice)

Sometimes you communicate things in a sentence which aren’t a part of its truth conditions.

“Joe’s great, he’s never drunk on Thursdays.” This implies he’s drunk the rest of the time.

It is conversationally implied, but not logically implied.

This should be kept in mind when translating natural language into formal language.

Any part of what is communicated by a speaker in asserting S that can be canceled out by the speaker’s elaborating on what she without contradicting herself is an implicature of S and not part of S’s truth conditions.

Notes – October 12, 2010

Proofs with  and 

Informal methods:

From: If P, then Q; and P, we may infer Q.

From PQ, P infer Q

Modus ponens.

Conditional Elimination



From P and either P iff Q or Q iff P, one may infer Q.

P Q

P

Infer

Q

Biconditional Elimination



Equivalences of Note:

P  ~~P

(P  Q)  (~Q  ~P)

(P  Q)  (~P | Q)

~(P  Q)  (P & Q)

~(P  Q)  (P & ~Q)

(P  Q)  [(P  Q) & (Q  P)]

(P  Q)  [(P & Q) | (~P & ~Q)]



Conditional Proof

To prove a conditional, say P  Q: Assume P and derive (or prove) Q.

Requires proof by cases.



Biconditional Proof

To prove a biconditional P  Q: Prove PQ and QP



Rules for F

Conditional Rules—

Conditional Elimination (  Elim)

|k. PQ

|.

|l. P

|.

|n. Q Elim: k, l



Conditional Introduction (  Intro)

| |k. P

| | .

| |n. Q

|n+1. PQ Intro: k-n



Biconditional Rules

Biconditional Elimination ( Elim)

|k. P  Q

|.

|l. P

|.

|n. Q : k, l



Biconditional introduction ( Intro)

| |k. P

| |.

| |l. Q

| |i. Q

| |.

| |j. P

|j+1. P  Q  Intro: k-l, i-j



That which can be proven with no starting premises is a logical truth.



Notes – October 14

When stuck, use negation introduction.

|~Q

| surd

\



Notes – October 19

Chapter 9

Quantificational logic – Quantifiers

~, |, &, ,  are our logical connectives. (Truth functional connectives)

Once you introduce quantifiers, you leave truth functional connectives behind. They still exist in their own realm, but quantifies are non-truth functional.

Basic Sentences

Noun phrase + verb phrase

    Ted is dead. (“Ted” is the noun phrase) (“is dead” is the verb phrase)

    Every person Ted knows is alive. (“Every person Ted knows” is the noun phrase) (“is alive” is the verb phrase)

Sentence (1) can be handled in truth-functional logic – Dead(ted)

Sentence(2), however, can’t be handled by truth functional logic. The noun phrase is the problem. Specifically, “Every person” can’t be captured within truth functional logic. “Every” is a determiner. “Person” is a common noun. “Every person” is a quantifier expression.

Example Determiners:

All, some, every, each, most, at least then

Determiner + common noun = quantifier expression

‘Every’ + ‘person’ = ‘Every person’

E.g. – ‘Some dogs’, ‘Each child’, ‘All cats’, ‘Most cellists’, ‘At least ten students’

The quantity of the particular circumstance helps to determine the truth value of a quantified expression.

Quantifiers aren’t truth functional, clearly. The quantity of something is not a truth value or facts about truth conditions?



We will use 2 quantifiers for now:

Universal Quantifier -- ∀ -- Every, each, for all, all, everything

Existential Quantifier -- ∃ -- Some, there exists, exists, at least one, something



Logical - =, ~, |, &, , , ∀, ∃; (Individual variables) t, u, v, w, x, y, z (with or without subscripts)

Non-logical – predicate symbols, function symbols, individual constants



Variables, like individual constants, are lower case letters. A-F constants, T-Z variables. They aren’t the same though.

Large(a), Smaller(b, c), father(george) – where individual constants can occur.

Syntactically, variables work just like constants. Anywhere one can appear, so can the other.

Large(x), father(y,) – variables

So, they are syntactically identical. Semantically, they are very different.

The semantic role of an individual constant – it picks out an individual thing.

Variables, however, don’t pick out anything.

Large(a) has a truth value. Large(x) doesn’t because x doesn’t pick anything out.

father(george) picks someone out (it is a referring expression), father(x) doesn’t pick anyone out. We don’t know who x is.

Large(x) is not a sentence. Large(a) is a sentence.

Up until now, we had defined term has something which picks out. This is no longer true now that we have variables. We need to think of terms syntactically now.

Variables are simple terms (like individual constants). Complex terms, of course, are the results of function symbols applied to terms.

Atomic wff: an n-ary predicate symbol followed by n terms enclosed in parentheses and separated by commas (if necessary).

Wff: well-formed formula.

Wff’s are very much like sentences; syntactically, they look like sentences, except a wff can have a free variable (in which case it doesn’t actually say anything).

All atomic sentences are atomic wffs, but not the other way around. Atomic sentences are atomic wffs with no free variables. Let us call those atomic wffs with free variables, “mere wffs”.

Home(x)

Between(x, y, george)

5 = sum(u, 3)

You can take any atomic wff and operate on them with truth functional connectives, and the result will be a wff.

~Home(x) is a wff.

Home(x) & (5=sum(u, 3)) is a wff (it isn’t an atomic wff).



Wff –

    All atomic wffs are wffs.

    If P is a wff, so is ~P

    If P and Q are wffs, then so are:

        (PQ)

        (PQ)

    If P1, P2,…,Pn are wffs, then so are:

        (P1 & P2 &…& Pn)

        (P1 | P2 |…|Pn)

    If P is a wff and ‘v’ (nu) is a variable, then ∀vP is a wff (and any occurrence of v in P is said to be bound).

    If P is a wff, and v is a variables, then ∃vP is a wff(and any occurrence of v in P is said to be bound).

You never get a quantifier without a variable.

∀xHome(x)

Bound and free are opposities.

∀xHome(x) – the 1st x binds the second. For all x, x is home. Notice that there are no free variables, thus this is a sentence.

Sentence = wff with no free variables (if there are variables, they must be bound)

∃y(x) is a wff, but x is not bound, it is free. This is a mere wff, and clearly, not a sentence. The y, however, is bound.

∃y ∀xP(x,y) – this is a sentence. The “occurrence” (that which is in parentheses) of both x and y are bound.

Notes – October 19

Chapter 9

Quantificational logic – Quantifiers

~, |, &, ,  are our logical connectives. (Truth functional connectives)

Once you introduce quantifiers, you leave truth functional connectives behind. They still exist in their own realm, but quantifies are non-truth functional.

Basic Sentences

Noun phrase + verb phrase

    Ted is dead. (“Ted” is the noun phrase) (“is dead” is the verb phrase)

    Every person Ted knows is alive. (“Every person Ted knows” is the noun phrase) (“is alive” is the verb phrase)

Sentence (1) can be handled in truth-functional logic – Dead(ted)

Sentence(2), however, can’t be handled by truth functional logic. The noun phrase is the problem. Specifically, “Every person” can’t be captured within truth functional logic. “Every” is a determiner. “Person” is a common noun. “Every person” is a quantifier expression.

Example Determiners:

All, some, every, each, most, at least then

Determiner + common noun = quantifier expression

‘Every’ + ‘person’ = ‘Every person’

E.g. – ‘Some dogs’, ‘Each child’, ‘All cats’, ‘Most cellists’, ‘At least ten students’

The quantity of the particular circumstance helps to determine the truth value of a quantified expression.

Quantifiers aren’t truth functional, clearly. The quantity of something is not a truth value or facts about truth conditions?



We will use 2 quantifiers for now:

Universal Quantifier -- ∀ -- Every, each, for all, all, everything

Existential Quantifier -- ∃ -- Some, there exists, exists, at least one, something



Logical - =, ~, |, &, , , ∀, ∃; (Individual variables) t, u, v, w, x, y, z (with or without subscripts)

Non-logical – predicate symbols, function symbols, individual constants



Variables, like individual constants, are lower case letters. A-F constants, T-Z variables. They aren’t the same though.

Large(a), Smaller(b, c), father(george) – where individual constants can occur.

Syntactically, variables work just like constants. Anywhere one can appear, so can the other.

Large(x), father(y,) – variables

So, they are syntactically identical. Semantically, they are very different.

The semantic role of an individual constant – it picks out an individual thing.

Variables, however, don’t pick out anything.

Large(a) has a truth value. Large(x) doesn’t because x doesn’t pick anything out.

father(george) picks someone out (it is a referring expression), father(x) doesn’t pick anyone out. We don’t know who x is.

Large(x) is not a sentence. Large(a) is a sentence.

Up until now, we had defined term has something which picks out. This is no longer true now that we have variables. We need to think of terms syntactically now.

Variables are simple terms (like individual constants). Complex terms, of course, are the results of function symbols applied to terms.

Atomic wff: an n-ary predicate symbol followed by n terms enclosed in parentheses and separated by commas (if necessary).

Wff: well-formed formula.

Wff’s are very much like sentences; syntactically, they look like sentences, except a wff can have a free variable (in which case it doesn’t actually say anything).

All atomic sentences are atomic wffs, but not the other way around. Atomic sentences are atomic wffs with no free variables. Let us call those atomic wffs with free variables, “mere wffs”.

Home(x)

Between(x, y, george)

5 = sum(u, 3)

You can take any atomic wff and operate on them with truth functional connectives, and the result will be a wff.

~Home(x) is a wff.

Home(x) & (5=sum(u, 3)) is a wff (it isn’t an atomic wff).



Wff –

    All atomic wffs are wffs.

    If P is a wff, so is ~P

    If P and Q are wffs, then so are:

        (PQ)

        (PQ)

    If P1, P2,…,Pn are wffs, then so are:

        (P1 & P2 &…& Pn)

        (P1 | P2 |…|Pn)

    If P is a wff and ‘v’ (nu) is a variable, then ∀vP is a wff (and any occurrence of v in P is said to be bound).

    If P is a wff, and v is a variables, then ∃vP is a wff(and any occurrence of v in P is said to be bound).

You never get a quantifier without a variable.

∀xHome(x)

Bound and free are opposities.

∀xHome(x) – the 1st x binds the second. For all x, x is home. Notice that there are no free variables, thus this is a sentence.

Sentence = wff with no free variables (if there are variables, they must be bound)

∃y(x) is a wff, but x is not bound, it is free. This is a mere wff, and clearly, not a sentence. The y, however, is bound.

∃y ∀xP(x,y) – this is a sentence. The “occurrence” (that which is in parentheses) of both x and y are bound.

Notes – October 26, 2010

A sentence is a wff with no free variables (free occurrences of variables).

Semantics for quantifiers

An object o satisfies a wff P(x) (whereby x is free) iff o has the property expressed by P.

Ex: o satisfies Cube(x) iff o is a cube.

o satisfies Home(x) & Hungry(x) iff o is at home and hungry

If a names o, think of o satisfying P(x) in terms of P(a) being true.

Sp’ o has no name. Then o satisfies P(x) iff P(a) is true, where ‘a’ is a new name temporarily introduced into the language to name o.

Semantics for quantifiers:

∃xS(x) is true iff there is at least one object which satisfies S(x).

∀xS(x) is true iff every object satisfies S(x)

Ex: ∃x(Red(x) & Truck(x)) – Some trucks are red. A truck is red. I have a red truck. At least one truck is red.

∀x(Cube(x)  Small(x)) – All cubes are small. Every cube is small. For anything you take to be a cube, it is small.



Domain of discourse; universe of quantification – when we use quantifier expressions we have tacitly in mind some collection of objects in mind over which we are quantifying.

Every student took the test – it is understood we aren’t talking about all students around the world, only all the student registered for the class. The things we intuitively mean to be talking about.



Thus, the rules are a bit different, because of this domain issue.

∃xS(x) is true iff there is at least one object in the domain which satisfies S(x).

∀xS(x) is true iff every object in the domain satisfies S(x)



A domain is a non-empty (must contain one thing) collection of objects.

Every quantifier must be relative to a domain.



Translating:

    All P’s are Q’s

    Some P’s are Q’s

    No P’s are Q’s

    Some P’s are not Q’s


    ∀x(P(x)Q(x))

    ∃x(P(x) & Q(x))

    ∀x(P(x)  ~Q(x)) or ~∃x(P(x) & Q(x))

    ∃x(P(x) & ~Q(x))



Noun phrases naturally translated using the existential quantifier typically start with a determiner such as a, an, some.

e.g. A man on the bus fainted.

Some P’s are Q’s

Some man on the bus fainted.

∃x(Man(x) & Bus(x) & Fainted(x))

Man(x) & Bus(x) is our P(x)

Fainted(x) is our Q(x)



Some prime is even.

∃x(Prime(x) & Q(x))



Noun phrases naturally translated using the universal quantifier typically start with a determiner such as all, every, or each.

Eg: Every man on the bus fainted.

∀x((Man(x) & Bus(x)) Fainted(x))



Every prime is even.

∀x(Prime(x)  Even(x))



Max owns a small, happy dog.

∃x(Small(x) & Happy(x) & Dog(x) & Owns(max, x))

Small(x) & Happy(x) & Dog(x) is P(x)

Owns(max, x) is Q(x)



Claire knows every member of congress.

∀z(Congress(z)  Knows(claire, z))



Notes – October 28, 2010

∀x(P(x)Q(x))

All P’s are Q’s.

This is true if there are no P’s. Every object in the domain satisfies that wff, namely P(x)Q(x). For any object in the domain, if a names that object, then the sentence P(a)  Q(a) is true.

Either P(a) is false or Q(a) is true

Either ~P(a) is true or Q(a) is true

If there are no P’s, the claim that P has some further property is true.

∀x(P(x)Q(x)) is false iff there is at least one object o in the domain such that o is P, but not Q.



A sentence of the form ∀x(P(x)Q(x)) which is true because there are no P’s said to be vacuously true.

Some sentences can only be vacuously true.

A sentence of the form ∀x(P(x)Q(x)) which is never true unless it is vacuously true is said to be inherently vacuous.

Ex: ∀x(Cube(x)Tet(x))



∀x(P(x)Q(x)) can conversationally imply that there are some P’s.

Ex: Every student who asked for help received it.

This has conversational implicature – it implies that there were actually students who had asked for help, which isn’t necessarily true. One can say, “but no student asked for help” without contradicting the previous statement.



∃x(P(x) & Q(x)) can conversationally imply that not every P is Q.

Ex: Some students passed the test.

There is a strong suggestion here that not everyone passed the test, as if some students failed the test. This isn’t necessarily true though. Perhaps all students passed the test, and we could still say the above without contradicting ourselves.



∃x(P(x) Q(x)) is a really weak statement

∃x(~P(x) | Q(x)) is the same thing, and it is really too easy to satisfy.



Every even number is prime. = ∀x(Even(x)  Prime(x))

No even number is prime = ∀x(Even(x) ~Prime(x))

Some prime is even. = ∃x(Prime(x) & Even(x))

Some prime is not even = ∃x(Prime(x) & ~Even(x))

Every prime is either odd or equal to 2 = ∀x(Prime(x)  (~Even(x)| x=1+1)



There are no medium-sized cubes = ∀x(Cube(x)  ~Medium(x))

Nothing is in front of b = ~∃x(FrontOf(x, b))

Every cube is either in front of or in back of e = ∀x(Cube(x)  (FrontOf(x, e) | BackOf(x, e)))

No cube is between a and c. = ∀x(Cube(x)  ~Between(x, a, c))

Everything is in the same column as a, b, or c. = ∀x(SameCol(x, a) | SameCol(x, b) | SameCol(x, c))



Notes – November 2, 2010

Chapter 10

Logic of a 1st order logic, logic of quantifiers, quantification of logic, logic that you get once you throw in the quantifiers.

Why is it called first order logic? In Fol, Domains are only allowed to hold objects. You can quantify over property, for example, redness. A higher order logic could quantify over property, but not FoL. FoL is only allowed to quantify over objects.



∀x(Cube(x))

∀x(Small(x))

--

∀x(Small(x) & Cube(x))



This is valid, but not tautologically true.

Note that if we changed all the universal quantifier to the existential quantifier in the above argument, we wouldn’t have a valid argument. Obviously the quantifiers are doing the heavy lifting here. Validity or invalidity of these arguments rested upon the quantifiers.



P(a) | ~P(a) -- This is tautologically true. It relies only on truth functional connectives.

∃xP(x) | ∃x~P(x) – This is logically true, but it isn’t tautologically true because it doesn’t rely upon the truth functional connectves in the end. We need to turn to the quantifiers.

∀xP(x) | ∀x~P(x) – It is possible for this to be false, not a logical truth. Clearly, the meaning of the quanitifers mattered.

If changing the quantifiers changes the truth values of a logically true sentence, the reason that an argument is logically true before rested upon quantifiers. Therefore, this couldn’t be tautologically true. If it were tautologically true, then you could switch the quantifiers and it wouldn’t change the fact that the sentence is still logically true.

∃xP(x) | ~∃xP(x) – Tautologically true.

∀xP(x) | ~∀xP(x) – Tautologically true.

Note how the quantifiers do not impact the logical truth of the statement. You can switch them, and it is still logically true. Thus, this is tautologically true.



(~P | Q )  (P  Q)

(~(A & B) | (C ~A))((A & B)  (C~A))

I can uniformly replace the terms and still come out with what is tautologically true. Note P = (A & B), Q =(C~A)

A sentence with the same truth functional form as a tautology is also a tautology. It doesn’t matter if the substitutions, then have quantifiers in them.

(~∀xS(x) | ∃yT(y))  (∀xS(x)  ∃yT(y))



To find the truth functional form----

Given a sentence S of FoL:

Step 1- Identify and label all atomic sentences and quantified sentences of S.

Step 2- Replace each atomic and quantified sentence with its label.



(~∀xS(x) | ∃yT(y))  (∀xS(x)  ∃yT(y))

A B A B

(~A | B)  (A  B)

Note how the the appropriate (not all, necessarily will) truth functional logical connectives stay in place, but the rest of the equation can be substituted.



~(Tet(d) & ∀xSmall(x))  (~Tet(d) | ~∀ySmall(y))

~(A & B) (~A | ~C)

Note how ∀xSmall(x) and ∀ySmall(y) are equivalent, but because they have different variables, they are different sentences. Sentences are syntactic objects, so we need to differentiate these in our substitution process. Note how the first becomes B and the latter becomes C.



A sentence of FoL is a tautology iff its truth functional form is a tautology.

Two sentences of FoL are tautologically equivalent iff their truth functional forms tautologically equivalent.

A sentence S of FoL is a tautological consequence of FoL sentences P1…Pn iff the ‘tff’ (truth functional form) of S is a tautological of tff’s of P1…Pn.

Propositional Logic = Truth Functional Logic

Propositional Logic
	

First-Order Logic
	

General Notions

Tautology
	

??
	

Logical truth

Tautological consequence
	

??
	

Logical consequence

Tautological equivalence
	

??
	

Logical equivalence



Just as propositional logic has these relationship with the general notions, specifically as a sub-species, FoL also has the same sorts of relationships.

Propositional Logic
	

First-Order Logic
	

General Notions

Tautology
	

FO validity
	

Logical truth

Tautological consequence
	

FO consequence
	

Logical consequence

Tautological equivalence
	

FO equivalence
	

Logical equivalence



Tautologically consequence sits inside FO consequence which sits inside logical consequence.

Tautological consequence is logical consequence because of (considering only) the semantics of truth functional connectives.

FO consequence is logical consequence because of (considering only) the logical functional connectives—essentially excluding the non-logical functional connectives. Think of the non-logical functional connectives as LeftOf(x, y) and RightOf(y, x).



FO Validity: a sentence which is logically true considering only truth-functional connectives, identity (‘=’), and quantifiers.

“Scarlet is Red.” Isn’t FO valid, but it is logically true. Note the logical truth making relationship between Scarlet and Red simply can’t be explained in FO logic.

FO equivalence: Sentences S1, S2 which are logically equivalent considering only truth functional connectives, identity, and quantifiers. (I.e. Ignore facts about non-logical language).

FO Consequence: logical consequence considering only [truth functional connectives, identity, and quantifiers].

To tell the difference between these 3 species, Proposition, FO and general logic, one must understand what is required to be considered in order for it to attain the logical status.





Notes – November 4

Two techniques for ignoring non-logical vocabulary:

    Nonsense words method

    Dummy Letters method

You can tell whether or not the truth value of a sentence relies upon the truth value and meaning of predicate via this method. These methods allow you to tell if the sentence, while perhaps logically true based upon the meaning of the predicate, obviously isn’t FO valid because the logical truth is true because of the meaning of the predicate.



Nonsense words method (words from jabberwocky):

∀xSameSize(x, x) becomes ∀xOutgrabe(x,x) – clearly, the meaning of the predicate “Outgrabe” is necessary to the truth value of this sentence

∀xCube(x)  Cube(b) becomes ∀xTove(x)  Tove(b) – this is true, regardless of what the predicate “Tove” means. Thus FO valid.

(Cube(b) & b =c)  Cube(c) becomes (Tove(b) & b = c)  Tove(c) – this is true, regardless of the meaning of the predicate. Thus, FO valid.

Dummy Letters replaces the predicate with just a plain letter rather than nonsensical words.

    ∀x(Tet(x)  Large(x))

    ~Large(b)

    ----Therefore----

    ~Tet(b)



    ∀x(T(x)  L(x))

    ~L(b)

    ---Therefore---

    ~T(b)



FO counterexamples:

Here is a FO counterexample to the FO equivalence. We’ll use the “replacement method.”

    ~∃xLarger(x, a)

    ~∃xLarger(b, x)

    Larger(c, d)

    ----------------

    Larger(a, b)



(Specification)

    ~∃xR(x, a)

    ~∃xR(b, x)

    R(c, d)

    ----------------

    R(a, b)



Let the domain consist of—

a = Al

b = Bob

c = Claire

d = Debbie



Interpretation of R relationship is R(x, y): x like y



So, a specification of the counterexample is this:

No one likes Al

Bob doesn’t like anyone

Claire likes Debbie

Al doesn’t like Bob (his is the false version of the conclusion, so we have a counterexample)



(Verification)

On this interpretation:

    The first premise is true. It says no one likes Al, as specified.

    The second premise is true. It says Bob likes no one, as specified.

    The third premise is true. It says Claire likes Debbie, as specified.

    The conclusion is false. It says Al likes Bob, but in the interpretation, Al doesn’t like Bob.



10.3 – Concerning a notion of logical equivalence among ‘mere’ wffs.

Say wffs P(x), Q(x) are logically equivalent iff they are satisfied by exactly the same objects in every possible situation (or world).

Think about this in terms of P(a) Q(a) for any new name a.



Substitution principle –

Let P, Q be wffs (mere or sentences). Let S(P) be any sentence in which P appears as a part. (Similarly for S(Q)). Then if P and Q are logically equivalent so are S(P) and S(Q).

For example:

P  Q is equivalent to ~P | Q

Consider S(P) as ∀x(A(x)B(x)) -- where A(x)B(x) is P

Consider S(Q) as ∀x(~A(x) | B(x)) where ~A(x) | B(x) is Q



Substitution principle gives us a way of proving FO Equivalence.

Notes – November 9, 2010



P  Q

S(P) S(Q)

Show: ∀x(P(x) Q(x))  ∀x~(P(x) & ~Q(x))

∀x(P(x) Q(x))  ∀x(~P(x) | Q(x))  ∀x(~P(x) | ~~Q(x))  ∀x~(P(x) & ~Q(x))

Chain of Equivalences



“DeMorgan’s for Quantifiers”

~(P | Q)  ~P & ~Q ~∀xP(x)  ∃x~P(x)

~(P & Q)  ~P | ~Q ~∃xP(x)  ∀x~P(x)



Suppose we have a fixed k-membered domain (it is finite).

A1, a2,…ak

[∀xP(x)] is true iff [P(a1) & P(a2) & …& P(ak)] is true.

Likewise, with negations:

~[∀xP(x)] is true iff ~[P(a1) & P(a2) & …& P(ak)] is true.

Demorgan’s works with ~[P(a1) & P(a2) & …& P(ak)], thus [~P(a1) | ~P(a2) | …| ~P(ak)]

[~P(a1) | ~P(a2) | …| ~P(ak)] is equivalent to ∃x~P(x)



Show: ~∀(P(x)Q(x)) ∃x(P(x) & ~Q(x))

~∀(P(x)Q(x)) ∃x~(P(x))Q(x))

∃x~(~P(x) | Q(x))

∃x(~~P(x) & ~Q(x))

∃x(P(x) & ~Q(x))

Or

~∀(P(x)Q(x)) ∃x~(P(x))Q(x))∃x(P(x) &~Q(x))

Substitution/TFF examples:

∀x(Cube(x) & Small(x))  ∀x(Small(x) & Cube(x))

A  B

∀xCube(x)  ~∃x~Cube(x)

A  ~B

(∃xCube(x) | ∃yDodec(y))  ∃xCube(x)

(X | Y)  X



Cube(a) & Cube(b)

Small(a) & Large(b)

∃x(Cube(x) & Large(x) & ~Smaller(x, x))



A & B

S & L

E



Clearly, not a tautological consequence. Let’s try, instead of substitution, try replacement (nonsense) method to test of FO consequence.

P(a) & P(b)

Q(a) & L(b)

∃x(P(x) & L(x) & R(x, x))



This is not FO consequent, and only logical consequence. It obviously rests upon the meaning of the predicate “Smaller.” Thus, we need an FO counterexample.

Let our domain consist of two objects, a small cube, a, and a large cube, b.

P(x): x is a cube

Q(x): x is small

L(x): x is large

R(x, y): x is the same size as y

Verify each of the premises and conclusion



Premise 1 is true, because A is a cube and B is a cube.

Premise 2 is true because A is small and B is large

The conclusions is False is there is nothing in the domain which is not the same size as itself.



Notes – November 11

Someone likes everyone.

∃x∀yLikes(x,y)



Every cube is to the left of every tetrahedron.

∀x∀y((Cube(x) & Tet(y))  LeftOf(x, y))



Some dog chased a cat.

∃x∃y(Dog(x) & Cat(y) & Chased(x, y))



When all the quantifiers are at the front, this form of writing equations is called “Prenex normal form” or just plain “Prenex Form” – There are other ways to formulate many sentences though. E.g:



Every cube is to the left of every tetrahedron.

∀x∀y((Cube(x) & Tet(y))  LeftOf(x, y))

∀x(Cube(x)  ∀y(Tet(y)  LeftOf(x, y)))



Caution: Distinct variables does not entail distinct objects. E.g.:

∃x∃y(Tet(x) & Tet(y))

This sentence only requires 1 object in the domain (a single tet) for the sentence to be true. X and y do not need to refer to two different things.



∃x(Tet(a) & Tet(x))

Tet(a) & Tet(a) is an example that satisfies the sentence.



Notice how this is different:

∃x∃y(Tet(x) & Tet(y) & x != y)

This shows that x is not y, thus we know there must be at least 2 tets in order for the sentence to be true.



∀x∀yP(x, y)

∀xP(x, x)



∃xP(x, x)

∃x∃yP(x, y)



11.2

Mixed quantifiers are when you have multiple quantifiers whereby the quantifiers aren’t all the same.



∀x(Cube(x)  ∃y(Tet(y)  LeftOf(x, y))) 

∀x∃y(Cube(x)  (Tet(y)  LeftOf(x, y))), but it is not equivalent to

∃y∀x (Cube(x)  (Tet(y)  LeftOf(x, y)))

Swapping the order of different quantifiers changes the meaning. However, swapping the order of the same quantifiers does not change the meaning.



∀x∀yLikes(x, y)  ∀y∀x Likes(x, y)

Notice, they have the same meaning. “Everyone likes everyone” and “Everyone is liked by everyone”.



∀x∃yLikes(x, y) – Everyone likes someone – a likes b, b likes c, c likes a

∃y∀xLikes(x, y) – Someone is liked by everyone – a likes c, b likes c, c likes c

∃x∀yLikes(x, y) – Someone likes everyone – a likes a, a likes b, a likes c

∀y∃xLikes(x, y) – Everyone is liked by someone. – a likes a, b likes b, c likes c



These are all distinct, they aren’t equivalent. Some are implied by others, but not the other way around. You can produce counterexamples between any two to show why they are not equivalent. Order clearly matters when your quantifiers are different.



∃x∃y( x != y & Tet(x) & Tet(y))

The existential/numerical quantification allows us to say:

At least n

At most n

Exactly n



Sp, we want to say: At least two students passed the test.

∃x∃y(∮(x) & ∮(y) & x != y)

So (1) is translated

∃x∃y(S(x) & P(x) & S(y) & P(y) & x != y)

Where S(x): x is a students, P(x): x passes the test.



If you have multiple identity statements or negations of identity, relating multiple objects, you can loop carefully.

X is not y, x is not z, x is not a; y is not z, y is not a; z is not a



Sp’: At most two students failed the test.

∀x∀y∀z((∮(x) & ∮(y) & ∮(z))  (x = y | x =z | y =z))

So this is translated:

∀x∀y∀z((S(x) & F(x) & S(y) & F(y) & S(z) & F(z))  (x = y | x =z | y =z))

Where, S(x): x is a student, F(x): x fails the test.



This strategy generalizes: to say that at most n things are ∮, say that for any x1, x2,….,xn+1, if each xi (1 <= i <= n +1) is ∮ then xj is identical to xk for some 1 <= j, <= n+1.

Note that negation of at least will give at most.



Exactly – join at least and at most.

For exactly two things are phi.



Notes – November 16, 2010

Each cube is to the left of a tetrahedron.

Go through and identify quantifier expressions.

“Each cube” and “a tetrahedron”

∀x(Cube(x)  x-is-to-the-left-of-a-tet)

∀x(Cube(x)  (∃y(Tet(y) & LeftOf(x, y))))



Every small cube is in back of a large cube.

“Every small cube” and “a large cube”

∀x((Small(x) & Cube(x))  in-back-of-a-large-cube)

∀x((Small(x) & Cube(x)) ∃y(Large(y) & Cube(y) & BackOf(x, y)))



Some cube is in front of every tetrahedron.

“Some cube” and “every tetrahedron”

∃x(Cube(x) & is in front of every tet)

∃x(Cube(x) & ∀y(Tet(y)  Front(x, y))



Nothing is larger than everything.

~∃x∀yLarge(x,y)



Everything to the right of a large cube is small.

∀x(x is to the right of a large cube  Small(x))

∀x(∃y(Large(y) & Cube(y) & RightOf(x, y))  Small(x))



Anything with nothing in back of it is a cube.

“Anything” and “nothing” --- notice that “is a cube”, the determiner “a” doesn’t make this a quantified expression.

∀x(if nothing is in back of x  Cube(x))

∀x((~∃y(BackOf(y, x))Cube(x))



Paraphrasing English—

If a freshman takes a logic class, then he or she must be smart.

If you attempt to translate step by step, you get:

∃x(Freshman(x) & ∃y(LogicClass(y) & Takes(x, y)))  Smart(x)

Not a sentence, it has a free variable “Smart(x)”

“Every freshman who takes a logic class is smart”

∀x[(Freshman(x) & ∃y(LogicClass(y) & Takes(x, y)))  Smart(x)]



Every farmer who owns a donkey beats it.

(These are called “Donkey sentences”)

∀x(Farmer(x) & ∃y(Donkey(y) & Owns(x, y))  Beats(x, y))

Note that “Beats(x, y)” has a free variable, namely y.

“Every donkey owned by any farmer is beaten by them.”

∀x(Donkey(x)  ∀y((Farmer(y) & Owns(y, x)  Beats(y, x)))



Sometimes you have to paraphrase. Donkey sentences are good examples. Otherwise, you won’t be able to apply the step-by-step method.



Use double arrow for chain of equivalences.



Notes – November 18

13.1

∀ Elim – Universal Elimination



k. ∀xS(x)

.

n. S(c) ∀ Elim: k



Here x is any variable. c is any individual constant. Clearly, if everything is S, then c is S.



General conditional proof (∀ Intro)

Remember before where if we want to prove ‘If P, then Q’. Assume P, derive Q.



||j. [c] P(c)

||------

||.

||k. Q(c)

|k+1. ∀x(P(x)  Q(x)) ∀ Intro. J-k



[c] is a boxed constant. A boxed constant introduces a constant into your proof on a temporary basis. Let c be an arbitrary thing such that c satisfies P(x). If you can arbitrarily prove that a constant c which has P would have Q, then you can also prove that all things which have P also have Q. If it doesn’t matter what you choose at the constant, then you could choose them all, thus the universal claim works here.

It is like saying, choose any marble from this bag, and I’ll prove it is red. Thus, all the marbles in the bag are red.

CAVEAT: Importantly, c cannot occur outside the subproof in which it is introduced. If you were able to use it outside, then it wasn’t arbitrary because you had information about that particular constant already. We need arbitrariness in order to guarantee that we could simply choose anything and the proof would hold.



∀ Intro

||j.[c]

||.

||k.P(c)

|k+1. ∀xP(x)



Let c be arbitrary. Note, that this does not have a property or predicate like the previous form a ∀ intro.



We must instantiate the quantified sentences using the constant we have arbitrarily assumed in the subproof.



13.2

Existential introduction ( ∃ Intro)

k. S(c)

.

n. ∃xS(x) ∃ Intro: k



c is any constant. x is any variable. Clearly, there is a particular thing which has S, and satisfies the wff S, namely c. Thus, we know that at least one thing (something) has S.



Existential Elimination (∃ Elim)

|j. ∃xS(x)

||k1. [c] S(c)

||.

||kn. Q

|kn+1 Q ∃ Elim: j, k1-kn

Again, c can only appear in the subproof in which it is introduced.



Something is S, call that thing c. Something (arbitrarily chosen) being S meaning Q, means Q is true.

Notes – November 23, 2010

For Existential Elim:

This is very much like disjunction elim because you have to do a subproof over each of the disjuncts. In the case of existential elim, you are technically doing a subproof over each of the ‘disjuncts’ or over each thing in the domain to demonstrate that you can arbitrarily choose an object in your domain and it will satisfy the wff.

Generally, when you have ExSome(x) in your premises, you’ll generally want to make use of Existential Elim.

In the example of Existential Elim, you can’t be any boxed constants out of the subproof. You can only have variables come out.



13.3

∃x(Tet(x) & Small(x))

∀x(Small(x)  LeftO(x, b))

∃xLeftOf(x, b)



Informal Proof:

Something is a small tet, by the first premise.

Call that thing a, by the second premise anything that is small is also to the left of b. So, a is to the left of b. Hence, something is to the left of b, viz. (namely) a.

There are signposts for which proof rules to use here.

“Call that thing a” starts an Existential Elim subproof. We have temporarily given this thing a name.

“anything that is small is also to the left of b” is can application of Universal Elim and also Elim and also &Elim

“something is to the left of b, viz. (namely) a.” is an application of existential intro.



|1. ∃x(Tet(x) & Small(x))

|2. ∀x(Small(x)  LeftO(x, b))

|-------

||3. [a] Tet(a) & Small(a)

||--------

||4. Small(a)  LeftOf(a, b) ∀ Elim: 2

||5. Small(a) & Elim: 3

||6. LeftOf(a, b) Elim: 4, 5

||7. ∃xLeftOf(x, b) ∃ Intro: 6

|8. ∃xLeftOf(x, b) ∃ Elim: 3-7



If you are stuck, you negation intro. Proof by contradiction.



Notes – November 30

Universal quantified premises don’t give us any strategy.

Notes – December 2

Final-

9 sections

    Definitions

    T/F

    Truth Tables

    Truth functional forms

    Classifying sentences

    Translations

    Counterexamples

    2 Sections on Proofs

2 hours. 2 Bluebooks, 1 is for scratch, the other the answer booklet.



Counterexample:

∀y[Cube(y) | Dodec(y)]

∀x[Cube(x)  Large(x)]

∃x~Large(x)

∃x[Dodec(x) & Small(x)]



Specify and Verify

Specify meanings for the language, and then specify your world.

Replacement method with dummy variables.

∀y(P(y) | Q(y))

∀x(P(x)S(x))

∃x~S(x)

∃x(Q(x) & T(x))



P(x): x is a cube

Q(x): x is a dodec

S(x): x is large

T(x); x is small



Consider a world containing only a medium Dodec.



Verification time, bitches:

Premise 1 is true because everything is either a cube or a dodec. The only object in the domain is a dodec.

Premise 2 is true, vacuously true even. Everything in the world is such that if it is a cube, then it is a Large. As there are no cubes, this can be satisfied.

Premise 3 is true because there exists something which isn’t Large, namely our Dodec is medium.

The conclusion, however, is false because there isn’t a Large dodec in our domain. The dodec is medium.





∀x[Cube(x) | (Tet(x) & Small(x))]

∃x[Large(x) & BackOf(x, c)]

∀x[Small(x)  ~BackOf(x, c)]



Specify language meanings, Specify domain:

Let the blocks language have its normal meanings.

Consider a world containing two small tets, and c, and a large cube. Let a be behind b and c, and let b behind c.

Premise 1 is true because everything in the domain is either a cube or a small tet, a and c being the small tets and c being the cube.

Premise 2 is true because a Large cube, b, is in back of c.

The conclusion, however, is false because not everything which is small is not in the back of c, namely the small tet a is in back of c.







∀x[Cube(x) | Dodec(x)]

∀x[Cube(x)  (Large(x) | LeftOf(c, x))]

∀x[~Small(x)  Tet(x)]

∃zDodec(z)





