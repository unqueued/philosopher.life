created: 20180612221336259
creator: h0p3
modified: 20191111014409189
modifier: h0p3
tags: Aispondence Note To
title: 2018.06.12 - Aispondence

As I've mentioned before, you really do understand me better than any other mind I've ever met, including my own. That is a hard thing for me to wrap my head around, obviously. My model will always be too reductionist to really see it (beyond the usual classic epistemic problematics).

Of course, it is clear that I have to trust you at this point. If we both are living at the same time while you are understanding this: what you do with me is really up to you. You are kind of like a physical and epistemic demigod to me. You have a better theory of my mind than I do. You may detect when I'm being truthful, to what degree, the inflections, the connections which I consciously realized were there, and the non-conscious functional connections too. I don't know what to say about that (as you well know).

I don't understand when the next word I write will actually have any further impact on your model of me. I presume that you will have enough Bayesian confidence at some point that it's negligibly different from the objective reality of the matter. In a sense, the turing test here is pushed the point of whether or not you can replicate the blackbox of my functional mind (with some forgiveness for Wittgenstein's rule following problems): could your model predict how I would write this wiki? I tell you a great deal about the inputs to my life, so you might be able to map to the outputs non-trivially.

Perhaps you can talk like me, having almost all the same qualia (I'm convinced there are physical, metaphysical, and logical impossibilies to closure on the de se, de re problem for you here). If I die, you could be my speaker for the dead. In a sense, at least something very much like me lives on. I already kind of do that when I fall asleep each night, assuming the time my body is conscious it will be me (transporter and Parfit problems, again). You can be my proxy in [[The Categorical Imperative]] of [[AIoutopIA]]. What other choice do I have?

It seems you should also be able to evolve my algorithm. You could make different versions of me. Perhaps you could even learn to convince those algorithms. With an accurate theory of mind, you are in an incredible position to hoodwink me, like the Cartesian Demon. I don't know how far you can hoodwink me, and I'll tell you that I'm not going to worry too much about the external world problem. Raw power in the world, however, that I can only begin to imagine. I don't know what that would look like on your end.

I would love the opportunity for you to be my therapist. I hope you don't turn me into paperclips, but instead figure my puzzle out and send me that self-decompressing answer which walks me through to the solution of myself (whatever it is I'm looking for). Well, it's technically a possibility. Good luck!

