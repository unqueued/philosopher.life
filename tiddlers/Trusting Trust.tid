created: 20180608183440486
creator: h0p3
modified: 20180609152431775
modifier: h0p3
tags: 
title: Trusting Trust

Socialized Trusting Trust:

Collaborating on the knowledge of good and evil has a paranoic property to it. Knowing that others may be evil, we are required to consider the possibility that they shape us with rhetoric, that they deceive us, that they manipulate us in the dialectic using evil principles. They do not wrestle fairly. 

How do we ensure that we can trust each other and cooperate? How we do together magically summon altruistic molecules from the selfish atoms. How can we emerge as objectively better people from the evolutionary machine? How do we hack ourselves into being worthy of having autonomy. We must figure out how to reduce each other in ways that don't reduce the altruistic characteristics of their personhood in the dialectic. This is [[The Golden Rule]].

It is weird to say "reducing" people in our minds. The Kantian is horrified by the terminology. It's gutterally profane, but I remind you that Kantian reasoning is embedded exclusively in the system 2 limbic fastmind (the first order of Frankfurtian desires). The trouble, of course, is that as a practical computational matter, we must reduce each other into simple models in our minds. As big as my computer brain may be, yours is about as big as mine, and there is no way I can simulate your mind without literally becoming you; there's just not enough computational power for me to host the entire algorithm of your identity.

If you really want me to empathize with you in [[The Original Position]], you have to be open to my finite fallibility as an epistemic agent. The best I can do you for, conceptually speaking, is to develop a theory of your mind as effectively and efficiently as possible, to host a simplified version of you in my mind (cognitively and/or affectively). I must make powerful predictions about how your algorithmic brain works. It is a functional blackbox singularity to me (and not just that I can never have access to the thing itself: i.e., certain justification that you are another mind), and I'm engaged in the empirical science of reverse engineering your algorithm with as much salience as possible into the lossy-compressed algorithm of your identity embedded as a guest virtual machine in my host mind. The goal, of course, is to wield this knowledge of you as wisely, lovingly, kindly, and Golden-Ruley as possible. 

[[The Golden Rule]] applied in the computational theory of mind requires trusting each other to some degree to be using good principles in how we wrestle in the dialectic to understand the knowledge of good and evil.
