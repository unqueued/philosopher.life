created: 20181021043119753
creator: h0p3
modified: 20181023192900319
modifier: h0p3
tags: [[Prompted Introspection]]
title: 2018.10.20 - Prompted Introspection: Lie Detection Superpower

!! If you suddenly gained the ability to tell whether someone was lying, would you use it?

Teacher, grandmaster, Diogenesocrates:

Gutturally, I consider myself to be a very poor lie detector. I think autists are especially vulnerable to liars. I feel like I have to cognitively work in overdrive to detect lies; I must ascribe a theory of mind to others necessary to make sense of that lie. When I am successful, I've usually found it by modeling a person with prescriptive egoistic motivation. I truly hate being lied to. I have worked very hard to be honest. In so many ways, I have experienced the costs of honesty. If my goal were mere pleasure, I'd be a radically more evil human; confabulating your way to being the hero of your own story is easy.

I'm already fairly misanthropic. I grow to despise most human specimens the more I get to understand who they really are. My [[hope]] in and for the human species barely exists, but I respect the dignity of personhood (it's Reason!). I understand the liar cannot change everything all at once. Repentance is a process. Doing the 180 is only the beginning; you've gotta restoratively walk back to the right trail as best you can.

I have long been interested in the notion of a [[Living Lie Detector]]. As always, what is a lie? I consider deception to be a more exact normative notion, being at the center of what is wrong with direct bald-faced lying. Deception is complex, and I do not know what it would mean to detect it. I suggest the ability might actually be almost entirely useless.

Imagine, if you will, a binary light. If they are deceptive, then it turns on for me. If they aren't, then it's off. That sounds like a simple reliable indicator for the externalist. Of course, deception might also come in [[dok]], and if there are spectrums and contextual notions to deception (and it appears there are), then perhaps our detector provides us far more than this binary information. So, what information does my deception-detector present to me?

It appears there is a difference between knowing //that// someone is being deceptive and knowing //how// they are being deceptive. Imagine I begin with a lie to myself, but make a bunch of valid (though not sound) inferences which are based upon this lie. I have to go back through the chain of reasoning to eventually find the origin of that deception. I suggest that almost all agents deceive themselves, and they even forget eventually that they've deceived themselves. They don't even the remember to original lie, and now they are stuck with a bottomless chain built on a pointless foundation. Does my ability detect this, and in what respects? 

Problematically, it is highly likely the case that every [[fff]] is deceiving themselves to some [[dok]]. Obviously, there is a trusting trust problem here, but it also means that my binary reliable indicator might just be permanently on. If it is to provide meaningful information, then it will have to not only tell me //that// someone is being deceptive but also //how//. Ah, but this indicator is crazy powerful. I suggest that it borders on being equivalent to simply having a model of someone else's mind; it might just be knowing what they know they know. 

I would use it. If I had the superpower to just know the truth, I might have a chance of succeeding in helping Humanity survive and thrive. I have no idea what the means to that end are yet. 

Ayyyyy...I'm workin' here!