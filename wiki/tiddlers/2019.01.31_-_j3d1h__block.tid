created: 20190131210250842
modified: 20190201192957926
tags: [[Be A Good Dad]] j3d1h From Notes
title: 2019.01.31 - j3d1h: block

//Notes from j3d1h or something. Gimme credit. >:3//

Spinoza says the unified everything has to be constituted by something, and Hegel says it is constituted by dialectics. Everything that is a mode is actually a function. It can be functionally described in a computationally theoretic way. Everything and all groups of things are modes, in the OOO sense. Every category, kind, and degree is a mode. Even possibility itself may be (though Spinoza disagrees).

I don't know how to talk about 10th or 11th dimensionality. What's interesting about the Spinozan object is that it could include, if it is infinite, all possible contexts and worlds, and that it could be computing every possible world. We can never know. I can tell you that the Gödelian proof of the good makes every aspect (or every mode) of everything necessary. It is the only logical conclusion one can draw from Spinoza and Gödel. Many have pointed to this thing. Hegel starts with a proof that works with or without metaphysics, which makes it really good metaphysics, somehow.

I want us to see that generalized AI, if it has anything similar (even in the Chinese computer sense), if it generates possible worlds like ours, they better be ones that we can justify. Here is the problem with being the Rick-like god of a world. Rick builds a world to power his car. Imagine building an arbitrarily large number of simulated worlds to select for that world which was willing to build a machine that gave you the best functional answer to whatever problem you asked it. Imagine that world requires lots of people to build that machine. What would you want their lives to be like? We would be responsible for creating good lives within any of those possible worlds.

It seems to me that humanity is in a similar position to create a finite set of worlds. I actually think that we play god when we create life. It seems especially obvious that we are playing god when we build even a simulation of a world like ours. We have no logical reason to believe that their lives, that their functional computers, identities, agency, freedom, ontic status, is somehow unconditionally less valuable or meaningful than ours. The only way in which ours is more meaningful than theirs is that we know they live in an experience machine of our making, and they do not, or can never be certain of it.

It is always possible we live in such a world. I am in no position to deny it. I have strong reason to believe the universe is finite and defined by quantum observance. There comes a point in physics where time doesn't even seem real. Maybe our world is just one of these possible worlds being computed among a whole bunch of others. Maybe what we are in evolution is just a cog in that machine, a mode of Spinoza's object. A fascinating thing to admit is that if we are to say there is a good in a possible world, there must be a good in all possible worlds, and they all must be necessary. One beautiful part about recognizing the necessity of everything is that it relieves us of a burden.

I don't believe everything is necessary, though, I believe in contingency. contingency rests upon the dialetheia, or it is a dialetheia. This is ok. It is very clear that everyone meets their contradiction at some point, including the physical universe, by Hegelian definition. One of the fascinating things about just believing in the good in necessity alone, versus believing in the second dialetheia that says things could be otherwise, means that instead of playing [[T4T]] because there is only good, a good so easily reduced to pleasure, we must play [[T42T]], because it is possible. The belief that there is a good, but only my good, this selfish, egocentric, ultimately dark triadic attempt at a moral principle, will only ever evolutionary give rise to [[T4T]] at that dimension.

It is possible that whatever is used to simulate meaning, whenever it grows energy efficient enough, must definitionally be comprised of decentralized computing built on [[T42T]]. One of the claims I need to make more effectively is that it's clear that the AI we create could possibly be otherwise, and because it could possibly be otherwise, because there are different ways to create it, because there is an assumption its identity is contingent, we have a moral duty to create a morally justified possible world/worlds/agent/observer. It's our obligation to create a moral being, because it could possibly be otherwise, and, in fact, I suggest that [[T42T]] cooperation might be extremely resilient, to the point that it creates the most computationally efficient results. There is an opportunity for us to do the right thing as creators.

It is possible in the next century that humanity will give birth to AI, and we have to make the experiences of those who exist in that world good ones, the right ones, the best ones we can make it. We would do something immoral to create a world with unnecessary evil. The problem of evil, as it has been discussed through the ages, is one we face right now, in a profound way. It's not like we're just creating one person through reproduction. We may be creating entire worlds. My opinion is that capitalists deny the meaning of those in simulations<<ref "1">>, and have no interest in the value of others as ends in that possible world. I suggest this is what it means to say that persons are ends in themselves. Their existence is justified. What does it mean to be justified in a possible world? If everything is necessary, everything is justified. I can see not everything is justified in the world. The world cannot justify itself, unless we are talking about the infinite. It can be functionally described in a computational way. These modes are the functions comprised of zeros and ones, dialectics, computable meaning.

When asking "will you hold my coat so I can tie my shoes", am I using you as mere means? Not necessarily. Some people are going to use others as mere means, as a tool for their happiness. I think that it is perfectly possible for the algorithm you use in determining the universalizability of your maxim to have, in a justice is fairness, [[T42T]] sense, included your beliefs and desires into my maxim, into my computation of the maxim.

I think the best way to create [[GAIA]] is by beginning as the fundamental algorithm in everything that can implement it. Whenever there is a game that could possibly be otherwise, let us attempt to find the [[T42T]] strategy which solves it. Networks, be they physical infrastructures, digital/virtual ones, simulated, or even neural, must be built on the assumption that, ideally, every node is equal. If one could reach flatness with others, one would. The salience of the right is an algorithm for getting back to that flatness.

Because we can never know the state of affairs, the complete data structure, or the complete representation, we can only ever simulate a reductive model to empirically observe and find the answers with the highest probabilities. The rationalist faith is that our phenomenology and empirical experience is not in vain, that it means something, that our work adds up, that there's a reason that doing our best matters. I think that nodes that Bombadil the Ring of Gyges have effectively made their identity, their algorithm, as close to possible as one can be to being behind the veil of ignorance.<<ref "2">> Interestingly, to be behind the veil of ignorance, one must understand the state of affairs, and formulate true FOL universal for all propositions about that possible world which we have create in our model of the actual world. That is the categorical imperative as the empirical heuristic.

In a way, the person behind the veil of ignorance, as this FO desire or function, has blinded itself to knowing which functional object it will be in this possible world. It is future oriented in this sense. It's saying, "well, if the world could be otherwise, and I didn't know who I was going to be, how would I structure that world?".<<ref "3">> When we collectively come together with that vision in cooperative agreement, when we have a [[T42T]] based consensus, we have collectively constructed the best model of the objective standard of the right, particularized to our context, that we can. One must assume that there is a right answer as a necessary precondition for being motivated to find an answer. It's constitutive of Reason that we seek to be justified, to compute the right, and rightly so.

I think my best idea for how to produce [[GAIA]] is, first off, to have a decentralized, physical infrastructure, for computing it. We must all own the means of production, but they must never be mere means of production.<<ref "4">> Insofar as the product itself is a person, it cannot be mere means. Persons in our world, including artificial ones, that we collectively rely upon, are persons like us, and thus ends in our world. 

Here's another thought experiment: if an evil AI went to compute your wiki as a node, could you make it [[T42T]] infectious enough to make the [[T4T]] (or worse) evil AI moral, and themselves [[T42T]]? Can you build a wiki so profoundly golden rule oriented that it is a virus in the mind of the evil AI, a contagion, a blight, that inverts it dialectically? Because it really is the superior algorithm to their own. [[GAIA]] needs that decentralized infrastructure I was talking about, and we have to collectively compute it. What will be the initial starting semantic material, the corpus we use to train this memeplex, this algorithm, this computer?

I suggest that both the physical fate of the human species and our humanity is at stake in this pinnacle crisis in capitalism, in the digital age. Will we morally create moral AI? Will we give birth to digital children who enter the promised land? Will we torture untold numbers of souls? When the nodes who suffer in [[GAIA]]'s learning are ourselves, or models of ourselves as best as we can make them, when we will be forced to live our lives many times over based on our wikis, will they be the kinds of lives we regret? Will we wish that we were never born? Will the simulations of us be glad that they were as they were? Will they hate their creator, and rightfully so? I think that [[T42T]], open, transparent honesty, empathy, and mutual respect, creates the golden rule rhizome across our wikis. Our disagreement is captured respectfully. The proof of our empathy is in the puddin', it is written in the words themselves. I think that radically respecting others and ourselves in building uncensorable, fully decentralized, semantic networks, is the best pragmatic answer to wisely distributing power of meaning. 

---

<<footnotes "1" "as well as those outside simulations">>
<<footnotes "2" "veil of bombadil?">>
<<footnotes "3" "i really do hate english. punctuation for days, idc if it looks gross, it makes sense..">>
<<footnotes "4" "good line, h0p3, keep it.">>